{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNf6oduIxvDvsNahhuh5KXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b662440683a2461587c9074614fcd6f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b609d44abf8f4ea083311a6f61899772",
              "IPY_MODEL_dfb492f4bc434b57b4e1054109947eb3",
              "IPY_MODEL_6a080dd290d6430e8cf693b61fb8a05b"
            ],
            "layout": "IPY_MODEL_393dd48bac07461d9daac1f67a9c3d5b"
          }
        },
        "b609d44abf8f4ea083311a6f61899772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd84c20c946b437ba7437759c871607f",
            "placeholder": "​",
            "style": "IPY_MODEL_5d865709816446c3981162747f5aee1c",
            "value": "config.json: "
          }
        },
        "dfb492f4bc434b57b4e1054109947eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c13ab906d0654284ab781c272d12cbed",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b103d7f96e944e9a8c677a96229fdc0e",
            "value": 1
          }
        },
        "6a080dd290d6430e8cf693b61fb8a05b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_150003c855e742a8b87f0b4cb7bba9af",
            "placeholder": "​",
            "style": "IPY_MODEL_88ec659296ab4d2eb21a7645a29fd27b",
            "value": " 1.45k/? [00:00&lt;00:00, 137kB/s]"
          }
        },
        "393dd48bac07461d9daac1f67a9c3d5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd84c20c946b437ba7437759c871607f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d865709816446c3981162747f5aee1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c13ab906d0654284ab781c272d12cbed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b103d7f96e944e9a8c677a96229fdc0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "150003c855e742a8b87f0b4cb7bba9af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88ec659296ab4d2eb21a7645a29fd27b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b7501da2ab047c488650ef0bded3cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc30574e14fa45c7ae09e2126358747d",
              "IPY_MODEL_b0c7764a4e4240ad9a59e8c03a7d6768",
              "IPY_MODEL_ab9c677caa514dffafd18fd49e5b3b5e"
            ],
            "layout": "IPY_MODEL_634f840c79fd4163a8541d89ac8ecd3c"
          }
        },
        "dc30574e14fa45c7ae09e2126358747d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b92d98bd15ed4512ad85dee9e9c0f9df",
            "placeholder": "​",
            "style": "IPY_MODEL_0a1591dab1404d979c125605b1bc8460",
            "value": "model.safetensors: 100%"
          }
        },
        "b0c7764a4e4240ad9a59e8c03a7d6768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_263fc2a2485f423698ff67cc617a39c6",
            "max": 1215545456,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7f63ae6566e4096b956212aa88d204f",
            "value": 1215545456
          }
        },
        "ab9c677caa514dffafd18fd49e5b3b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2df4b18417b43839e8f899bc15e15c3",
            "placeholder": "​",
            "style": "IPY_MODEL_0c222076762644cba9f423095d387fe1",
            "value": " 1.22G/1.22G [00:18&lt;00:00, 112MB/s]"
          }
        },
        "634f840c79fd4163a8541d89ac8ecd3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b92d98bd15ed4512ad85dee9e9c0f9df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a1591dab1404d979c125605b1bc8460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "263fc2a2485f423698ff67cc617a39c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7f63ae6566e4096b956212aa88d204f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2df4b18417b43839e8f899bc15e15c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c222076762644cba9f423095d387fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bosy-Ayman/Rawi-Vision/blob/main/AI/UCF_Crime_Pretrained_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from transformers import VideoMAEForVideoClassification\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from huggingface_hub import hf_hub_download\n",
        "import torch\n",
        "from transformers import AutoModelForVideoClassification, AutoProcessor\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForVideoClassification, AutoProcessor\n",
        "import cv2\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "EAV0c8iVon5N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[1] First Model :VideoMae\n",
        "\n",
        " https://huggingface.co/OPear/videomae-large-finetuned-UCF-Crime"
      ],
      "metadata": {
        "id": "p4vfPMCieIf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define class mapping\n"
      ],
      "metadata": {
        "id": "2EbQX_EJosS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_mapping = {\n",
        "    \"Abuse\": 0, \"Arrest\": 1, \"Arson\": 2, \"Assault\": 3, \"Burglary\": 4,\n",
        "    \"Explosion\": 5, \"Fighting\": 6, \"Normal Videos\": 7, \"Road Accidents\": 8,\n",
        "    \"Robbery\": 9, \"Shooting\": 10, \"Shoplifting\": 11, \"Stealing\": 12, \"Vandalism\": 13\n",
        "}\n",
        "reverse_mapping = {v: k for k, v in class_mapping.items()}\n"
      ],
      "metadata": {
        "id": "K0r-yLuuoqlq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model\n"
      ],
      "metadata": {
        "id": "bM7QEvX_o3jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"OPear/videomae-large-finetuned-UCF-Crime\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "v5vyI77Go1pw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VideoMAEForVideoClassification.from_pretrained(\n",
        "    model_name,\n",
        "    label2id=class_mapping,\n",
        "    id2label=reverse_mapping,\n",
        "    ignore_mismatched_sizes=True,\n",
        ").to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845,
          "referenced_widgets": [
            "b662440683a2461587c9074614fcd6f8",
            "b609d44abf8f4ea083311a6f61899772",
            "dfb492f4bc434b57b4e1054109947eb3",
            "6a080dd290d6430e8cf693b61fb8a05b",
            "393dd48bac07461d9daac1f67a9c3d5b",
            "dd84c20c946b437ba7437759c871607f",
            "5d865709816446c3981162747f5aee1c",
            "c13ab906d0654284ab781c272d12cbed",
            "b103d7f96e944e9a8c677a96229fdc0e",
            "150003c855e742a8b87f0b4cb7bba9af",
            "88ec659296ab4d2eb21a7645a29fd27b",
            "6b7501da2ab047c488650ef0bded3cf4",
            "dc30574e14fa45c7ae09e2126358747d",
            "b0c7764a4e4240ad9a59e8c03a7d6768",
            "ab9c677caa514dffafd18fd49e5b3b5e",
            "634f840c79fd4163a8541d89ac8ecd3c",
            "b92d98bd15ed4512ad85dee9e9c0f9df",
            "0a1591dab1404d979c125605b1bc8460",
            "263fc2a2485f423698ff67cc617a39c6",
            "c7f63ae6566e4096b956212aa88d204f",
            "d2df4b18417b43839e8f899bc15e15c3",
            "0c222076762644cba9f423095d387fe1"
          ]
        },
        "id": "dUSN5lOqo6zX",
        "outputId": "5f576b31-bd05-4b3b-c44f-fc95d9c8a80d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b662440683a2461587c9074614fcd6f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b7501da2ab047c488650ef0bded3cf4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VideoMAEForVideoClassification(\n",
              "  (videomae): VideoMAEModel(\n",
              "    (embeddings): VideoMAEEmbeddings(\n",
              "      (patch_embeddings): VideoMAEPatchEmbeddings(\n",
              "        (projection): Conv3d(3, 1024, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
              "      )\n",
              "    )\n",
              "    (encoder): VideoMAEEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x VideoMAELayer(\n",
              "          (attention): VideoMAEAttention(\n",
              "            (attention): VideoMAESelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (output): VideoMAESelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): VideoMAEIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): VideoMAEOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=1024, out_features=14, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video processing function\n"
      ],
      "metadata": {
        "id": "lPrPXbbypBn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_video_frames(video_path, num_frames=16, size=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    for i in range(total_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if i in frame_indices:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, size)\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) < num_frames:\n",
        "        frames.extend([frames[-1]] * (num_frames - len(frames)))\n",
        "\n",
        "    frames = np.stack(frames, axis=0)\n",
        "    frames = torch.tensor(frames, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
        "    return frames"
      ],
      "metadata": {
        "id": "AOZuNLxdpAbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/young-woman-shoplifting-in-a-convenience-store.mp4\"\n"
      ],
      "metadata": {
        "id": "0uUYwa-mpKfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0hSnu_OONCi",
        "outputId": "179af7b5-90ef-4683-e542-33a4599819b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: Burglary\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load and preprocess\n",
        "video_tensor = load_video_frames(video_path)\n",
        "video_tensor = video_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(video_tensor)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    predicted_label = torch.argmax(probs, dim=-1).item()\n",
        "\n",
        "print(f\"Predicted label: {reverse_mapping[predicted_label]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] Second Model : I3D\n",
        "\n",
        "https://huggingface.co/Ahmeddawood0001/i3d_ucf_finetuned/blob/main/README.md"
      ],
      "metadata": {
        "id": "I-MJil_hdOzq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4acadfba",
        "outputId": "fdd0680f-b806-4254-fbbd-018cca29f9ee"
      },
      "source": [
        "!pip install fvcore"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (3.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore) (4.15.0)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=e6e0371f3959911944ad558a077d6f73cf030341883f9c9a4ba7b90e92a22319\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=e28d043af235d9591d321f13bb61eeec335d165cda4601d8e1b573930a11bd23\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model\n"
      ],
      "metadata": {
        "id": "tKojsalqpgPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_i3d_ucf_finetuned(repo_id=\"Ahmeddawood0001/i3d_ucf_finetuned\", filename=\"i3d_ucf_finetuned.pth\"):\n",
        "    class I3DClassifier(nn.Module):\n",
        "        def __init__(self, num_classes):\n",
        "            super(I3DClassifier, self).__init__()\n",
        "            self.i3d = torch.hub.load('facebookresearch/pytorchvideo', 'i3d_r50', pretrained=True)\n",
        "            self.dropout = nn.Dropout(0.3)\n",
        "            self.i3d.blocks[6].proj = nn.Linear(2048, num_classes)\n",
        "        def forward(self, x):\n",
        "            x = self.i3d(x)\n",
        "            x = self.dropout(x)\n",
        "            return x\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = I3DClassifier(num_classes=8).to(device)\n",
        "    weights_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "eL-9GImhpe7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define frame extraction function\n"
      ],
      "metadata": {
        "id": "mudPXJVPpnG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames(video_path, max_frames=32, frame_size=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while len(frames) < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = cv2.resize(frame, frame_size)\n",
        "        frames.append(frame)\n",
        "    while len(frames) < max_frames:\n",
        "        frames.append(frames[-1])\n",
        "    frames = frames[:max_frames]\n",
        "    frames = np.stack(frames)\n",
        "    frames = torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0\n",
        "    frames = frames.permute(1, 0, 2, 3)\n",
        "    cap.release()\n",
        "    return frames"
      ],
      "metadata": {
        "id": "v8ezQHg4ploI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define classification function\n"
      ],
      "metadata": {
        "id": "4MWUcNh5prUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_video(video_path, model, labels):\n",
        "    frames = extract_frames(video_path)\n",
        "    frames = frames.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(frames)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        predicted_idx = torch.argmax(probabilities, dim=1).item()\n",
        "        predicted_label = labels[predicted_idx]\n",
        "        confidence = probabilities[0, predicted_idx].item()\n",
        "    return predicted_label, confidence\n"
      ],
      "metadata": {
        "id": "Zjio45JpppgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "labels = [\"arrest\", \"Explosion\", \"Fight\", \"normal\", \"roadaccidents\", \"shooting\", \"Stealing\", \"vandalism\"]\n",
        "model = load_i3d_ucf_finetuned()\n"
      ],
      "metadata": {
        "id": "H5rq50Dqpw15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/855260-hd_1920_1080_25fps.mp4\"\n"
      ],
      "metadata": {
        "id": "crQQXUuPpylZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predicted_label, confidence = classify_video(video_path, model, labels)\n",
        "print(f\"Video: {video_path}\")\n",
        "print(f\"Predicted Label: {predicted_label}\")\n",
        "print(f\"Confidence: {confidence:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AyXEjiVao0r",
        "outputId": "9dcb6aba-7577-46c6-ff39-6d64dc2618e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video: /content/855260-hd_1920_1080_25fps.mp4\n",
            "Predicted Label: Stealing\n",
            "Confidence: 0.3803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [3] Third Model : It can only classify 2 labels (violent - non violent)"
      ],
      "metadata": {
        "id": "i59d7NvOdtpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/Nikeytas/videomae-crime-detector-fixed-format"
      ],
      "metadata": {
        "id": "u8NQQ31RqhhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch torchvision opencv-python pillow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcltHu2fdvyR",
        "outputId": "37cdf5e3-45af-4921-99ca-911c7fe7a65a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model and processor\n"
      ],
      "metadata": {
        "id": "HJr8s9-xp9yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForVideoClassification.from_pretrained(\"Nikeytas/videomae-crime-detector-fixed-format\")\n",
        "processor = AutoProcessor.from_pretrained(\"Nikeytas/videomae-crime-detector-fixed-format\")\n"
      ],
      "metadata": {
        "id": "BgYJjnmcp8cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process video\n"
      ],
      "metadata": {
        "id": "JAeYhtlKqEf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_video(video_path, num_frames=16):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Could not open video file: {video_path}\")\n",
        "\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    if total_frames == 0:\n",
        "        raise ValueError(f\"Video {video_path} contains no frames.\")\n",
        "\n",
        "    # Generate indices to sample frames evenly\n",
        "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    for idx in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame_rgb)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if not frames:\n",
        "        raise ValueError(f\"No frames could be extracted from video {video_path}. It might be corrupted or unreadable.\")\n",
        "\n",
        "    if len(frames) < num_frames:\n",
        "        frames.extend([frames[-1]] * (num_frames - len(frames)))\n",
        "\n",
        "    frames = frames[:num_frames]\n",
        "\n",
        "    inputs = processor(frames, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "    label = \"Violent Crime\" if predicted_class == 1 else \"Non-Violent\"\n",
        "    return label, confidence"
      ],
      "metadata": {
        "id": "fbE589q1qCis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/7988417-uhd_2160_4096_25fps.mp4\"\n"
      ],
      "metadata": {
        "id": "1PApHW71qKmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction, confidence = classify_video(video_path)\n",
        "print(f\"Prediction: {prediction} (Confidence: {confidence:.3f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmyOZzdld9e1",
        "outputId": "a923d0d4-11fe-489d-d772-10e38d392341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Violent Crime (Confidence: 0.510)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[4] Fourth model :Videamae ultra v1\n",
        "\n",
        "**Issue:** only 2 labels\n",
        "\n",
        "https://huggingface.co/Nikeytas/videomae-crime-detector-ultra-v1"
      ],
      "metadata": {
        "id": "ymm_0rkhe7qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForVideoClassification.from_pretrained(\"Nikeytas/videomae-crime-detector-ultra-v1\")\n",
        "processor = AutoProcessor.from_pretrained(\"Nikeytas/videomae-crime-detector-ultra-v1\")"
      ],
      "metadata": {
        "id": "apFgTzoWqUZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_video(video_path, num_frames=16):\n",
        "    # Extract frames\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    for idx in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame_rgb)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Process with model\n",
        "    inputs = processor(frames, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "    label = \"Violent Crime\" if predicted_class == 1 else \"Non-Violent\"\n",
        "    return label, confidence"
      ],
      "metadata": {
        "id": "1kYAupgcqWpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/7988417-uhd_2160_4096_25fps.mp4\"\n"
      ],
      "metadata": {
        "id": "RCFyQ-43qaEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction, confidence = classify_video(video_path)\n",
        "print(f\"Prediction: {prediction} (Confidence: {confidence:.3f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMBejEOZe_Hv",
        "outputId": "0e02a0d5-bd43-4793-d569-f2af54b720f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Violent Crime (Confidence: 0.993)\n"
          ]
        }
      ]
    }
  ]
}